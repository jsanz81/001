{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceso Gaussiano\n",
    "\n",
    "* Entonces, los procesos gaussianos surgieron inicialmente cuando estábamos mirando la interpolación.\n",
    "* Queríamos una forma de interpolar entre puntos pero con cierta incertidumbre sobre nuestro valor.\n",
    "* Entonces, hagamos un resumen rápido de la interpolación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal as mn\n",
    "import seaborn as sb\n",
    "cs = ['#56d870', '#f9ee4a', '#44d9ff', '#f95b4a', '#3d9fe2', '#ffa847', '#c4ef7a', '#e195e2', '#ced9ed', '#fff29b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(xs):\n",
    "    return np.exp((xs + 10.5)**0.1) + np.sin(xs) / (xs + 1) + np.cos(2.5 * xs**0.5)**2\n",
    "\n",
    "xs = np.array([1, 2, 4, 5, 7, 9, 10])\n",
    "ys = fn(xs)\n",
    "x_fine = np.linspace(1, 10, 200)\n",
    "y_fine = fn(x_fine)\n",
    "\n",
    "plt.plot(xs, ys, 'o', label=\"Muestras\", markersize=5)\n",
    "plt.plot(x_fine, y_fine, label=\"Función\", color=\"k\", alpha=0.5, linestyle=\"-\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "y_linear = interp1d(xs, ys, kind=\"linear\")(x_fine)\n",
    "y_quad = interp1d(xs, ys, kind=\"quadratic\")(x_fine)\n",
    "y_cubic = interp1d(xs, ys, kind=\"cubic\")(x_fine)\n",
    "\n",
    "plt.plot(xs, ys, 'o', label=\"Muestras\", markersize=5)\n",
    "plt.plot(x_fine, y_fine, label=\"Funcion\", color=\"w\", ls=\"-\", alpha=0.5)\n",
    "plt.plot(x_fine, y_linear, label=\"Lineal\")\n",
    "plt.plot(x_fine, y_quad, label=\"Cuadratico\")\n",
    "plt.plot(x_fine, y_cubic, label=\"Cúbico\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que muchas opciones de interpolación diferentes. Nada se parece a lo que queremos. Ahora, aparte, creemos una matriz de covarianza altamente correlacionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(20)\n",
    "length = 50\n",
    "cov = np.exp(-(1 / length) * (x - np.atleast_2d(x).T)**2)\n",
    "sb.heatmap(cov);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    rvs = mn.rvs(cov=cov)\n",
    "    plt.plot(x, rvs, 'o-', ms=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "rvs = mn.rvs(cov=cov, size=200)\n",
    "rvs -= np.linspace(rvs[:, 0], rvs[:, -1], x.size).T  # Ayuda conceptual, no cómo lo harías realmente/\n",
    "\n",
    "mean = rvs.mean(axis=0)\n",
    "std = np.std(rvs, axis=0)\n",
    "\n",
    "plt.plot(x, mean, \"o-\", ms=5)\n",
    "plt.fill_between(x, mean + std, mean - std, alpha=0.4)\n",
    "for i in range(100):\n",
    "    plt.plot(x, rvs[i, :], ls=\"-\", c=\"k\", alpha=0.1, lw=0.5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el proceso de pseudo-GP anterior, queríamos interpolar 20 puntos (bueno, 18 puntos 'desconocidos' dado que los extremos son fijos), y lo hicimos dibujando desde un Gaussiano 20-D. Pero, obviamente, hay un número infinito de puntos a los que podríamos querer interpolar, por lo que dibujaríamos a partir de una Gaussiana de dimensión infinita. Esto es lo que significa \"no paramétrico\": dimensiones infinitas.\n",
    "\n",
    "Y dibujando una y otra vez podemos obtener una predicción media y una desviación estándar. Tenga en cuenta que esto depende de nuestra elección de covarianza. El cov que hemos utilizado tiene un solo parámetro, su escala de longitud. Cuanto más grande sea $l$, más suaves y menos cambiantes serán las predicciones. Este número que 'escogemos' se conoce como hiperparámetro. Y es algo en lo que tenemos que encajar al crear un médico de cabecera real. Y en términos de nomenclatura, pasando de un proceso MVGaussiano a uno Gaussiano no tenemos una media, tenemos una función media (porque podemos pedir la media en cualquier x). De manera similar, la matriz de covarianza depende de los puntos de datos de entrada (en nuestro ejemplo anterior no teníamos ninguno, solo arreglamos el primer y el último punto) y los puntos en los que queremos evaluar, por lo que tenemos una función de covarianza. La forma de la covarianza (es decir, teníamos la distancia al cuadrado) se llama núcleo (puedes tener diferentes funciones, la distancia al cuadrado es útil)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesos Gaussianos\n",
    "\n",
    "Interpolación con ``esteroides``. Para esta sección, necesitará sklearn. `pip install sklearn` si no lo tiene.\n",
    "\n",
    "El proceso gaussiano funciona entrenando un modelo, que se ajusta a los hiperparámetros del núcleo específico que proporciona. La dificultad está en saber qué kernel construir y luego dejar que el modelo entrene. Este núcleo esencialmente relaciona cómo cada punto de datos afecta las regiones en el espacio de parámetros.\n",
    "\n",
    "Todo lo que dice este núcleo es que si está interpolando en el punto x, que se encuentra entre el punto A y el punto B, determina cuánto contribuye A (pero tomando $\\exp(-(x-A)^2/(2\\sigma)^2 )$) y luego haga lo mismo para B. Obtiene un peso diferente para A y B, y lo usa para calcular cuál cree que es el valor en x. La parte de entrenamiento de esto es encontrar el mejor valor de $\\sigma$ en el núcleo que da los mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, ys, 'o', label=\"Muestras\", markersize=5)\n",
    "plt.plot(x_fine, y_fine, label=\"Funcion\", color=\"k\", ls=\"-\", alpha=0.5)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "kernels = [\n",
    "    RBF(length_scale=1.0, length_scale_bounds=(1.0, 1.01)), \n",
    "    RBF(length_scale=2.0, length_scale_bounds=(2.0, 2.01)),\n",
    "    RBF(length_scale=20.0, length_scale_bounds=(20.0, 20.01)),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10, 4), ncols=len(kernels), sharey=True)\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "for k, ax in zip(kernels, axes):\n",
    "    gp = GaussianProcessRegressor(kernel=k)\n",
    "    gp.fit(np.atleast_2d(xs).T, ys)\n",
    "    \n",
    "    y_mean, y_std = gp.predict(x_fine[:, None], return_std=True)\n",
    "\n",
    "    ax.plot(xs, ys, 'o', label=\"Samples\", markersize=5)\n",
    "    ax.plot(x_fine, y_fine, label=\"Function\", color=\"k\", ls=\"-\", lw=1, alpha=0.5)\n",
    "    ax.plot(x_fine, y_mean)\n",
    "    ax.fill_between(x_fine, y_mean + 2 * y_std, y_mean - 2 * y_std, alpha=0.5)\n",
    "    ax.set_title(str(k) + f\" {y_std.max():0.4f}\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se Puede ver a la izquierda que tenemos un modelo donde la escala de longitud es demasiado pequeña. A la derecha, es tan grande que aleja incluso los puntos de datos conocidos de donde pertenecen. Pero el medio se ve bastante bien.\n",
    "\n",
    "Lo que hacemos es ajustar los hiperparámetros para maximizar la probabilidad marginal condicionada a los hiperparámetros. No entraré en matemáticas, pero la probabilidad marginal tiene un término para qué tan bien se ajusta a los datos (por lo que la gráfica de la derecha tendría un puntaje horrible), un término de normalización y una penalización por complejidad (por lo que la gráfica de la mano izquierda modelo sería demasiado complejo porque su escala es demasiado pequeña y permite demasiada libertad).\n",
    "\n",
    "Pero esto solo habla de cómo entrenar hiperparámetros. Hay un montón de núcleos diferentes entre los que se puede elegir, y esa es la tarea realmente difícil. Si se usa procesos gaussianos profundos, también puedes hacerlo, pero eso va mucho más allá de este curso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.gaussian_process.kernels import ExpSineSquared as ES, RationalQuadratic as RQ\n",
    "\n",
    "kernels = [\n",
    "    C() * RBF(), \n",
    "    ES(periodicity=10),\n",
    "    C() * RQ() + RBF()\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10, 4), ncols=len(kernels), sharey=True)\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "for k, ax in zip(kernels, axes):\n",
    "    gp = GaussianProcessRegressor(kernel=k)\n",
    "    gp.fit(np.atleast_2d(xs).T, ys)\n",
    "    \n",
    "    y_mean, y_std = gp.predict(x_fine[:, None], return_std=True)\n",
    "\n",
    "    ax.plot(xs, ys, 'o', label=\"Muestras\", markersize=5)\n",
    "    ax.plot(x_fine, y_fine, label=\"Funcion\", color=\"k\", ls=\"-\", lw=1, alpha=0.5)\n",
    "    ax.plot(x_fine, y_mean)\n",
    "    ax.fill_between(x_fine, y_mean + 2 * y_std, y_mean - 2 * y_std, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=C() * RBF())\n",
    "gp.fit(np.atleast_2d(xs).T, ys)\n",
    "y_pred, sigma = gp.predict(np.atleast_2d(x_fine).T, return_std=True)\n",
    "\n",
    "upper, lower = y_pred + 1.96 * sigma, y_pred - 1.96 * sigma\n",
    "\n",
    "plt.plot(xs, ys, 'o', label=\"Muestras\", markersize=5)\n",
    "plt.plot(x_fine, y_fine, label=\"Funcion\", color=\"k\", ls=\"-\", alpha=0.5)\n",
    "plt.plot(x_fine, y_pred, label=\"GP\")\n",
    "plt.fill_between(x_fine, upper, lower, alpha=0.5, label=\"95% confianza\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesos Gaussianos con incertidumbre\n",
    "\n",
    "Conceptualmente, ¿qué pasa si nuestros puntos de datos tienen incertidumbre? Lo que estamos haciendo es agregar una matriz de covarianza adicional a la mezcla, donde para nuestra incertidumbre independiente será una matriz diagonal. Formalmente, esto se llama regularización de Tikhonov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "xs2 = np.array([1, 1.5, 2, 3, 4, 5, 7, 9, 10])\n",
    "ys2 = fn(xs2)\n",
    "\n",
    "err_scale = np.random.uniform(low=0.03, high=0.1, size=ys2.shape)\n",
    "err = np.random.normal(loc=0, scale=err_scale, size=ys2.shape)\n",
    "ys_err = ys2 + err\n",
    "\n",
    "plt.errorbar(xs2, ys_err, yerr=err, fmt=\"o\", label=\"Muestras\", markersize=5)\n",
    "plt.plot(x_fine, y_fine, label=\"Funcion\", color=\"k\", alpha=0.5, linestyle=\"-\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianProcessRegressor(alpha=err**2)\n",
    "gp.fit(np.atleast_2d(xs2).T, ys_err)\n",
    "y_pred, sigma = gp.predict(np.atleast_2d(x_fine).T, return_std=True)\n",
    "\n",
    "upper, lower = y_pred + 1.96 * sigma, y_pred - 1.96 * sigma\n",
    "\n",
    "plt.errorbar(xs2, ys_err, yerr=err, fmt=\"o\", label=\"Muestras\", color=\"#f95b4a\", markersize=5)\n",
    "plt.plot(x_fine, y_fine, label=\"Funcion\", color=\"k\", ls=\":\", alpha=0.5)\n",
    "plt.plot(x_fine, y_pred, label=\"GP\", color=\"#44d9ff\", ls=\"-\")\n",
    "plt.fill_between(x_fine, upper, lower, alpha=0.2, color=\"#44d9ff\", label=\"95% confidence\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los procesos gaussianos requieren sutileza en el entrenamiento, especialmente en el decapado del kernel correcto. Es un campo muy profundo y rico en el que fácilmente puede pasar todo un doctorado investigando."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
